# -*- coding: utf-8 -*-
"""ExploringData.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eLV-2gwTjhYAu_fVjavzJQVzBDBKWgu7
"""

!pip install datasets

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from datasets import Dataset

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Load the dataset
df = pd.read_csv('/content/drive/MyDrive/lyrics-data.csv')

from google.colab import drive
drive.mount('/content/drive')

df = df[df['language'] == 'en']

# Drop unnecessary columns
df.drop(['ALink', 'SName', 'SLink'], axis=1, inplace=True)
# Prepare the dataset for Hugging Face
# Take a subset for training
df = df[:500]
lyrics = df['Lyric'].tolist()
dataset = Dataset.from_dict({'text': lyrics})

vectorizer = CountVectorizer().fit(df['Lyric'])
tokens = vectorizer.get_feature_names_out()

tfidf_matrix = vectorizer.fit_transform(df['Lyric'])

cos_sim = cosine_similarity(tfidf_matrix)

def apply_gatt(attention_matrix):
    return attention_matrix * 1.2

attention_before = np.random.rand(len(tokens), len(tokens))
attention_after = np.random.rand(len(tokens), len(tokens))

cos_sim_gatt = apply_gatt(cos_sim)

from wordcloud import WordCloud

all_lyrics = ' '.join(df['Lyric'])

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_lyrics)

# Plot the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Lyrics')
plt.show()

from collections import Counter
import seaborn as sns

all_lyrics = ' '.join(df['Lyric'])

# Define a set of stopwords (you can add more words to this set if needed)
stopwords = set(STOPWORDS)

# Tokenize the lyrics and count the frequency of each word, excluding stopwords
words = [word for word in all_lyrics.split() if word.lower() not in stopwords]
word_freq = Counter(words)

# Get the most common words, excluding stopwords
common_words = word_freq.most_common(20)
words, counts = zip(*common_words)

# Plot the frequency distribution
plt.figure(figsize=(12, 6))
sns.barplot(x=list(words), y=list(counts), palette='viridis')
plt.xticks(rotation=45)
plt.title('Top 20 Most Common Words (Excluding Stopwords)')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.show()

import numpy as np
import seaborn as sns

# Calculate cosine similarity (as a placeholder for attention mechanism)
cos_sim = cosine_similarity(tfidf_matrix)

# Plot the heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(cos_sim, cmap='viridis')
plt.title('Cosine Similarity between Lyrics')
plt.xlabel('Lyrics')
plt.ylabel('Lyrics')
plt.show()

"""## Portuguse üî∞"""

from wordcloud import STOPWORDS

stopwords = set([
    'a', '√†', '√°', '√†s', 'agora', 'ainda', 'algu√©m', 'algum', 'alguma', 'algumas', 'alguns', 'ali', 'ampla', 'amplas',
    'amplo', 'amplos', 'ante', 'antes', 'ao', 'aos', 'apenas', 'apoio', 'ap√≥s', 'aquela', 'aquelas', 'aquele', 'aqueles',
    'aquilo', 'as', '√†s', 'at√©', 'atr√°s', 'bem', 'bom', 'cada', 'caminho', 'catorze', 'cedo', 'cento', 'certamente',
    'certeza', 'cinco', 'coisa', 'coisas', 'com', 'como', 'conselho', 'contra', 'contudo', 'custa', 'da', 'd√°', 'd√£o',
    'daquela', 'daquelas', 'daquele', 'daqueles', 'dar', 'das', 'de', 'debaixo', 'dela', 'delas', 'dele', 'deles', 'demais',
    'dentro', 'depois', 'desde', 'dessa', 'dessas', 'desse', 'desses', 'desta', 'destas', 'deste', 'destes', 'deve', 'devem',
    'dever√°', 'dez', 'dezanove', 'dezasseis', 'dezassete', 'dezoito', 'dia', 'diante', 'direita', 'diz', 'dizem', 'dizer',
    'do', 'dois', 'dos', 'doze', 'duas', 'e', '√©', 'ela', 'elas', 'ele', 'eles', 'em', 'embora', 'enquanto', 'entre', 'era',
    '√©s', 'essa', 'essas', 'esse', 'esses', 'esta', 'est√°', 'estamos', 'est√£o', 'estar', 'estas', 'est√°s', 'este', 'estes',
    'esteve', 'estive', 'estivemos', 'estiveram', 'eu', 'exemplo', 'fa√ßo', 'falta', 'favor', 'faz', 'fazeis', 'fazem', 'fazemos',
    'fazer', 'fazes', 'fez', 'fim', 'final', 'foi', 'fomos', 'for', 'foram', 'forma', 'foste', 'fui', 'geral', 'grande', 'grandes',
    'grupo', 'h√°', 'hoje', 'isso', 'isto', 'j√°', 'lado', 'lhe', 'lhes', 'lo', 'logo', 'longe', 'lugar', 'maior', 'maioria', 'mais',
    'mal', 'mas', 'me', 'mesma', 'mesmas', 'mesmo', 'mesmos', 'meu', 'meus', 'mil', 'minha', 'minhas', 'momento', 'muito', 'muitos',
    'na', 'nada', 'naquela', 'naquelas', 'naquele', 'naqueles', 'nas', 'nem', 'nenhuma', 'nessa', 'nessas', 'nesse', 'nesses', 'nesta',
    'nestas', 'neste', 'nestes', 'no', 'noite', 'n√≥s', 'nome', 'nos', 'nossa', 'nossas', 'nosso', 'nossos', 'num', 'numa', 'nunca',
    'o', 'obra', 'obrigada', 'obrigado', 'oitava', 'oitavo', 'oito', 'onde', 'ontem', 'onze', 'os', 'ou', 'outra', 'outras', 'outro',
    'outros', 'para', 'parece', 'parte', 'partir', 'pelo', 'pela', 'pelas', 'pelos', 'per', 'perante', 'perto', 'pode', 'podem',
    'poder√°', 'podia', 'p√µem', 'ponto', 'pontos', 'por', 'porque', 'porqu√™', 'posi√ß√£o', 'possivelmente', 'posso', 'pouca', 'poucas',
    'pouco', 'poucos', 'primeira', 'primeiras', 'primeiro', 'primeiros', 'prome', 'pr√≥pria', 'pr√≥prias', 'pr√≥prio', 'pr√≥prios', 'pr√≥ximo',
    'pr√≥ximos', 'puderam', 'p√¥de', 'qual', 'qualquer', 'quando', 'quanto', 'quarta', 'quarto', 'quatro', 'que', 'quem', 'quer', 'quereis',
    'querem', 'queremos', 'queres', 'quero', 'quest√£o', 'quieto', 'quinta', 'quinto', 'quinze', 'rela√ß√£o', 'sabe', 'sabem', 's√£o', 'se',
    'segunda', 'segundo', 'sei', 'seis', 'seja', 'sejam', 'sem', 'sempre', 'sendo', 'ser', 'seria', 'sete', 'seu', 'seus', 'sexta',
    'sexto', 'sim', 'sistema', 'sob', 'sobre', 'sois', 'somos', 'sou', 'sua', 'suas', 'tal', 'talvez', 'tamb√©m', 'tanta', 'tantas', 'tanto',
    't√£o', 'tarde', 'te', 'tem', 't√™m', 'temos', 'tendes', 'tendo', 'tenha', 'ter', 'terceira', 'terceiro', 'teu', 'teus', 'teve', 'tipo',
    'tive', 'tivemos', 'tiveram', 'toda', 'todas', 'todavia', 'todo', 'todos', 'trabalho', 'tr√™s', 'treze', 'tu', 'tua', 'tuas', 'tudo',
    '√∫ltima', '√∫ltimas', '√∫ltimo', '√∫ltimos', 'um', 'uma', 'umas', 'uns', 'vai', 'vais', 'v√£o', 'v√°rios', 'vem', 'v√™m', 'vens', 'ver',
    'vez', 'vezes', 'viagem', 'vindo', 'vinte', 'voc√™', 'voc√™s', 'vos', 'vossa', 'vossas', 'vosso', 'vossos', 'zero', 'e', 'a', 'o',
    'um', 'de', 'da', 'n√£o', '√©', 's√≥'
])

# Load the dataset
df = pd.read_csv('/content/drive/MyDrive/lyrics-data.csv')

df = df[df['language'] == 'pt']

# Drop unnecessary columns
df.drop(['ALink', 'SName', 'SLink'], axis=1, inplace=True)
# Prepare the dataset for Hugging Face
# Take a subset for training
df = df[:1000]
lyrics = df['Lyric'].tolist()
dataset = Dataset.from_dict({'text': lyrics})

all_lyrics = ' '.join(df['Lyric'])

words = [word for word in all_lyrics.split() if word.lower() not in stopwords]
word_freq = Counter(words)

common_words = word_freq.most_common(20)
words, counts = zip(*common_words)

plt.figure(figsize=(12, 6))
sns.barplot(x=list(words), y=list(counts), palette='viridis')
plt.xticks(rotation=45)
plt.title('Top 20 Most Common Words (Excluding Stopwords)')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.show()

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_lyrics)

# Plot the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Lyrics')
plt.show()































































